import os
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
import argparse
import re
import traceback
import time

# --- Configuration ---
DEFAULT_CACHE_DIR = "./binance_data_cache/"

# Expected columns and their properties for aggregate trades
EXPECTED_AGG_TRADES_COLUMNS_INFO = {
    'Price': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'Quantity': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'IsBuyerMaker': {'dtype': np.bool_, 'not_null': False},
}

# Expected columns and their properties for kline data (OHLCV + potential TAs)
EXPECTED_KLINE_BASE_COLUMNS_INFO = {
    'Open': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'High': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'Low': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'Close': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'Volume': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
}

# Regex to parse filenames generated by the new daily fetching/caching in utils.py:
# bn_aggtrades_SYMBOL_YYYY-MM-DD.parquet
AGG_TRADES_FILENAME_PATTERN = re.compile(
    r"^(?P<prefix>bn_aggtrades)_" # Matches the fixed prefix "bn_aggtrades_"
    r"(?P<symbol>[A-Z0-9]+)_"      # Matches the symbol (e.g., BTCUSDT)
    r"(?P<date>\d{4}-\d{2}-\d{2})" # Matches YYYY-MM-DD
    r"\.parquet$"                  # Matches ".parquet" extension
)

# Regex for kline filenames:
# bn_klines_SYMBOL_INTERVAL_YYYY-MM-DD_FEATURES.parquet
KLINE_FILENAME_PATTERN = re.compile(
    r"^(?P<prefix>bn_klines)_"     # Matches the fixed prefix "bn_klines_"
    r"(?P<symbol>[A-Z0-9]+)_"      # Matches the symbol (e.g., BTCUSDT)
    r"(?P<interval>[a-zA-Z0-9]+)_" # Matches the interval (e.g., 1h, 1m, 1d)
    r"(?P<date>\d{4}-\d{2}-\d{2})"     # Matches YYYY-MM-DD
    r"(?P<features>_([a-zA-Z0-9]+(_[a-zA-Z0-9]+)*))?" # Optional features part (e.g., _sma20_rsi14_volume)
    r"\.parquet$"                  # Matches ".parquet" extension
)


# Helper to convert Binance interval string to timedelta
def interval_to_timedelta(interval_str: str) -> timedelta:
    unit = interval_str[-1]
    value = int(interval_str[:-1])
    if unit == 'm':
        return timedelta(minutes=value)
    elif unit == 'h':
        return timedelta(hours=value)
    elif unit == 'd':
        return timedelta(days=value)
    elif unit == 'w':
        return timedelta(weeks=value)
    elif unit == 'M':
        # Approximate for months, could be more precise with calendar logic
        return timedelta(days=value * 30)
    else:
        raise ValueError(f"Unsupported interval unit: {interval_str}")


def parse_filename_for_metadata(filename):
    """
    Parses the filename (e.g., bn_aggtrades_BTCUSDT_2024-01-01.parquet
    or bn_klines_BTCUSDT_1h_2024-01-01_open_high.parquet)
    to extract symbol, date, data type, interval, and features.
    Returns a dictionary with metadata or None if parsing fails.
    """
    basename = os.path.basename(filename)

    # Try parsing as aggregate trades file
    match_agg = AGG_TRADES_FILENAME_PATTERN.match(basename)
    if match_agg:
        data = match_agg.groupdict()
        try:
            parsed_date = datetime.strptime(data['date'], '%Y-%m-%d').date()
            start_time = datetime.combine(parsed_date, datetime.min.time(), tzinfo=timezone.utc)
            end_time = datetime.combine(parsed_date, datetime.max.time(), tzinfo=timezone.utc) # End of day
            
            return {
                "data_type": "agg_trades",
                "symbol": data['symbol'],
                "start_time_utc": start_time,
                "end_time_utc": end_time,
                "prefix": data['prefix']
            }
        except ValueError:
            return None

    # Try parsing as kline file
    match_kline = KLINE_FILENAME_PATTERN.match(basename)
    if match_kline:
        data = match_kline.groupdict()
        try:
            parsed_date = datetime.strptime(data['date'], '%Y-%m-%d').date()
            start_time = datetime.combine(parsed_date, datetime.min.time(), tzinfo=timezone.utc)
            end_time = datetime.combine(parsed_date, datetime.max.time(), tzinfo=timezone.utc) # End of day
            
            features_str = data.get('features')
            parsed_features = []
            if features_str:
                normalized_list = features_str.strip('_').split('_')
                for f in normalized_list:
                    if f.startswith('sma') and f[3:].isdigit():
                        parsed_features.append(f"SMA_{f[3:]}")
                    elif f.startswith('ema') and f[3:].isdigit():
                        parsed_features.append(f"EMA_{f[3:]}")
                    elif f.startswith('rsi') and f[3:].isdigit():
                        parsed_features.append(f"RSI_{f[3:]}")
                    elif f == 'macd':
                        parsed_features.append('MACD')
                    elif f == 'adx':
                        parsed_features.append('ADX')
                    elif f == 'stochk':
                        parsed_features.append('STOCH_K')
                    elif f == 'atr':
                        parsed_features.append('ATR')
                    elif f == 'bbandsupper':
                        parsed_features.append('BBANDS_Upper')
                    elif f == 'ad':
                        parsed_features.append('AD')
                    elif f == 'obv':
                        parsed_features.append('OBV')
                    elif f.startswith('cdl'):
                        parsed_features.append(f.upper())
                    else: # For base OHLCV features like 'open', 'high', etc.
                        parsed_features.append(f.capitalize())
            
            base_ohlcv = ['Open', 'High', 'Low', 'Close', 'Volume']
            # Ensure base OHLCV are always considered expected, even if not explicitly in filename features string
            # This accounts for when --kline_features is not passed or only TAs are listed.
            for col in base_ohlcv:
                if col not in parsed_features:
                    parsed_features.append(col) # Add to the list to be checked

            return {
                "data_type": "kline",
                "symbol": data['symbol'],
                "interval": data['interval'],
                "start_time_utc": start_time,
                "end_time_utc": end_time,
                "prefix": data['prefix'],
                "features": parsed_features
            }
        except ValueError:
            return None
    
    return None

def validate_daily_data(filepath: str) -> tuple[bool, str]:
    """
    Performs various checks on a single daily Parquet cache file for aggregate trades OR klines.
    Returns True and a success message if all checks pass, False and an error message otherwise.
    """
    print(f"\n--- Checking File: {filepath} ---")
    
    df = pd.DataFrame()
    max_read_retries = 10
    read_retry_delay_sec = 0.2

    for i in range(max_read_retries):
        try:
            df = pd.read_parquet(filepath)
            print(f"  Successfully read Parquet file after {i+1} attempt(s). Shape: {df.shape}")
            if df.empty:
                return True, "DataFrame is empty after reading. (May be valid for periods with no trades/klines)."
            break
        except FileNotFoundError:
            print(f"  WARNING: File not found during read attempt {i+1}/{max_read_retries}. Retrying in {read_retry_delay_sec}s...")
            time.sleep(read_retry_delay_sec)
        except Exception as e:
            traceback.print_exc()
            return False, f"Could not read Parquet file (error type: {type(e).__name__}): {e}"
    else:
        return False, f"File did not become readable after {max_read_retries} attempts. File not found or persistently inaccessible."

    file_size_bytes = os.path.getsize(filepath)
    if file_size_bytes == 0 and not df.empty:
        return False, "File is 0 bytes but DataFrame was not empty after reading (inconsistent)."
    print(f"  File Size: {file_size_bytes / 1024:.2f} KB")


    metadata = parse_filename_for_metadata(filepath)
    if metadata:
        print(f"  Parsed Metadata: Data Type={metadata['data_type']}, Symbol={metadata['symbol']}, Start={metadata['start_time_utc']}, End={metadata['end_time_utc']}")
        if metadata['data_type'] == 'kline':
            print(f"    Interval={metadata['interval']}, Features={metadata['features']}")
    else:
        return False, f"Could not parse filename '{os.path.basename(filepath)}' for metadata. It does not match the expected pattern."

    all_checks_ok = True
    messages = []

    expected_cols_info = {}
    if metadata['data_type'] == 'agg_trades':
        expected_cols_info = EXPECTED_AGG_TRADES_COLUMNS_INFO
    elif metadata['data_type'] == 'kline':
        expected_cols_info = EXPECTED_KLINE_BASE_COLUMNS_INFO.copy()
        if 'features' in metadata and metadata['features']:
            for feature in metadata['features']:
                if feature not in expected_cols_info:
                    expected_cols_info[feature] = {'dtype': np.float64, 'not_null': False}

    if not isinstance(df.index, pd.DatetimeIndex):
        messages.append(f"Index is not a DatetimeIndex. Actual type: {type(df.index)}")
        all_checks_ok = False
    elif df.index.tz is None or str(df.index.tz) != 'UTC':
        messages.append(f"Index is not UTC-aware or not UTC. Actual tz: {df.index.tz}")
        all_checks_ok = False
    elif not df.index.is_monotonic_increasing:
        messages.append("Index (Timestamp) is not monotonically increasing.")
        all_checks_ok = False
    
    # 4. Column Presence and Unexpected Columns
    expected_cols_set = set(expected_cols_info.keys())
    actual_cols_set = set(df.columns)

    missing_cols = expected_cols_set - actual_cols_set
    extra_cols = actual_cols_set - expected_cols_set

    if missing_cols:
        messages.append(f"Missing expected columns: {missing_cols}")
        all_checks_ok = False
    if extra_cols:
        messages.append(f"WARNING: Found unexpected extra columns: {extra_cols}")

    # 5. Data Type, NaN, and Specific Value Checks
    for col_name, info in expected_cols_info.items():
        if col_name not in df.columns:
            continue

        expected_dtype = pd.Series([], dtype=info['dtype']).dtype
        actual_dtype = df[col_name].dtype
        
        if actual_dtype != expected_dtype:
            messages.append(f"Column '{col_name}' dtype is '{actual_dtype}' but expected '{expected_dtype}'.")
            all_checks_ok = False

        if info.get('not_null', False):
            nan_count = df[col_name].isnull().sum()
            if nan_count > 0:
                messages.append(f"Column '{col_name}' contains {nan_count} NaN/null values but is expected to be not-null.")
                all_checks_ok = False
        
        if info.get('check_positive', False) and pd.api.types.is_numeric_dtype(df[col_name]):
            if (df[col_name] <= 0).any():
                messages.append(f"WARNING: Column '{col_name}' contains zero or negative values but expected positive.")

    # 6. Timestamp Range Check
    if not df.empty:
        min_time_in_df = df.index.min()
        max_time_in_df = df.index.max()
        
        if min_time_in_df.tzinfo is None: min_time_in_df = min_time_in_df.tz_localize('UTC')
        if max_time_in_df.tzinfo is None: max_time_in_df = max_time_in_df.tz_localize('UTC')

        expected_start = metadata['start_time_utc']
        expected_end = metadata['end_time_utc']
        
        leeway = timedelta(seconds=1) 

        if metadata['data_type'] == 'kline':
            interval_td = interval_to_timedelta(metadata['interval'])
            
            if min_time_in_df < expected_start - leeway:
                messages.append(f"WARNING: First timestamp in K-line data ({min_time_in_df}) is slightly before expected start from filename ({expected_start}).")
            elif min_time_in_df > expected_start + interval_td: # Allow up to one interval start for potential missing first candle (though generally shouldn't happen for full day)
                messages.append(f"ERROR: First timestamp in K-line data ({min_time_in_df}) is significantly after expected start from filename ({expected_start}). Data might be missing from start.")
                all_checks_ok = False
            
            # For klines, the last candle's *open time* should correspond to the interval just before the end of the day.
            # E.g., for 1h, last candle starts at 23:00:00 (for day ending 23:59:59.999999)
            # The expected last open time is the start of the day + 23 * interval (for 1h) or end of day - interval
            expected_last_kline_open_time = expected_end.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1) - interval_td
            # Adjust if the end of the day is exactly 23:59:59.999999 and interval is not a divisor of a day
            # For daily data, the last candle's OpenTime should be (start of next day - interval)
            # Example: 2025-05-01 23:59:59.999999 (expected_end)
            # 2025-05-02 00:00:00 (start of next day)
            # - 1h interval = 2025-05-01 23:00:00.
            
            # Use `expected_end.floor('D') + timedelta(days=1) - interval_td` for robust calculation
            expected_last_kline_open_time = pd.Timestamp(expected_end.date() + timedelta(days=1), tz=timezone.utc) - interval_td

            if max_time_in_df < expected_last_kline_open_time - leeway:
                messages.append(f"ERROR: Last timestamp in K-line data ({max_time_in_df}) is significantly before expected end of daily coverage based on interval ({expected_last_kline_open_time}). Data might be incomplete for the day.")
                all_checks_ok = False
            elif max_time_in_df > expected_last_kline_open_time + leeway:
                messages.append(f"WARNING: Last timestamp in K-line data ({max_time_in_df}) is slightly after expected end of daily coverage based on interval ({expected_last_kline_open_time}).")
            
            if df.shape[0] > 1:
                time_diffs = df.index.to_series().diff().dropna()
                # Check if all time differences are approximately the interval duration
                # Allow a small margin for float precision
                if not ((time_diffs >= interval_td - timedelta(milliseconds=10)) & (time_diffs <= interval_td + timedelta(milliseconds=10))).all():
                    messages.append(f"ERROR: K-line intervals are not consistently {metadata['interval']}. Max gap: {time_diffs.max()}, Min gap: {time_diffs.min()}. Data might be incomplete or corrupted.")
                    all_checks_ok = False

        else: # For agg_trades data
            if min_time_in_df < expected_start - leeway:
                messages.append(f"WARNING: First timestamp in data ({min_time_in_df}) is slightly before expected start from filename ({expected_start}).")
            elif min_time_in_df > expected_start + timedelta(minutes=5):
                messages.append(f"ERROR: First timestamp in data ({min_time_in_df}) is significantly after expected start from filename ({expected_start}). Data might be missing from start.")
                all_checks_ok = False
            
            if max_time_in_df > expected_end + leeway:
                messages.append(f"WARNING: Maximum timestamp in data ({max_time_in_df}) is slightly after expected end from filename ({expected_end}).")
            elif max_time_in_df < expected_end - timedelta(minutes=5):
                messages.append(f"ERROR: Last timestamp in data ({max_time_in_df}) is significantly before expected end from filename ({expected_end}). Data might be missing from end.")
                all_checks_ok = False
            
            if df.shape[0] > 1:
                time_diffs = df.index.to_series().diff().dropna()
                max_gap = time_diffs.max()
                if max_gap > timedelta(hours=1):
                    messages.append(f"WARNING: Large time gap detected in Aggregate Trades data: {max_gap}. Data might be incomplete for the day.")


    if all_checks_ok:
        return True, "All checks passed."
    else:
        return False, "One or more checks FAILED:\n" + "\n".join([f"  - {msg}" for msg in messages])

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Check integrity of cached Parquet tick or kline data files.")
    parser.add_argument(
        "filepath",
        nargs='?',
        default=None,
        help="Path to a specific Parquet file to check. If not provided, all .parquet files in CACHE_DIR will be checked."
    )
    parser.add_argument(
        "--cache_dir",
        default=DEFAULT_CACHE_DIR,
        help=f"Directory containing cache files. Default: {DEFAULT_CACHE_DIR}"
    )
    args = parser.parse_args()

    cache_directory = args.cache_dir
    if not os.path.isdir(cache_directory):
        print(f"ERROR: Cache directory '{cache_directory}' does not exist.")
        exit(1)

    if args.filepath:
        if not os.path.isfile(args.filepath):
            print(f"ERROR: Specified file '{args.filepath}' does not exist.")
            exit(1)
        if not args.filepath.endswith(".parquet"):
            print(f"ERROR: Specified file '{args.filepath}' is not a .parquet file.")
            exit(1)
        
        is_valid, msg = validate_daily_data(args.filepath)
        print(msg)
        if not is_valid:
            exit(1)
    else:
        print(f"Checking all .parquet files in directory: {cache_directory}")
        found_files = []
        for root, _, files in os.walk(cache_directory):
            for f in files:
                if f.endswith(".parquet"):
                    found_files.append(os.path.join(root, f))

        if not found_files:
            print("No .parquet files found in the directory or its subdirectories.")
        else:
            print(f"Found {len(found_files)} Parquet files.")
            overall_passed = 0
            overall_failed = 0
            for filepath in found_files:
                is_valid, msg = validate_daily_data(filepath)
                print(msg)
                if is_valid:
                    overall_passed += 1
                else:
                    overall_failed += 1
            print(f"\n--- Overall Summary ---")
            print(f"Files Checked: {len(found_files)}")
            print(f"Passed: {overall_passed}")
            print(f"Failed: {overall_failed}")
            if overall_failed > 0:
                exit(1)