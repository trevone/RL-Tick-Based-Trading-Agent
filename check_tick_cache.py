import os
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
import argparse
import re
import traceback

# --- Configuration ---
DEFAULT_CACHE_DIR = "./binance_data_cache/" # Adjusted default cache directory to match utils.py's default

# Expected columns and their properties (based on output of utils.py's fetch_continuous_aggregate_trades)
EXPECTED_COLUMNS_INFO = {
    # Note: 'id' and 'isBestMatch' might not be explicitly present in the final DataFrame saved by utils.py
    # because it keeps only ['Price', 'Quantity', 'IsBuyerMaker'].
    # Adjust EXPECTED_COLUMNS_INFO to match the *actual* columns saved by utils.py if you want strict checks.
    # For now, I'm adapting to the utils.py output: Price, Quantity, IsBuyerMaker
    'Price': {'dtype': np.float64, 'not_null': True, 'check_positive': True}, # Renamed from 'price'
    'Quantity': {'dtype': np.float64, 'not_null': True, 'check_positive': True}, # Renamed from 'qty'
    'Timestamp': {'dtype': 'datetime64[ns, UTC]', 'not_null': True, 'monotonic_increasing': True}, # Renamed from 'time'
    'IsBuyerMaker': {'dtype': np.bool_, 'not_null': False}, # Renamed from 'isBuyerMaker'
}

# Regex to parse filenames like: bn_aggtrades_BTCUSDT_2024-01-01_to_2024-01-03.parquet
# This regex is specifically designed for the format generated by utils.py's fetch_continuous_aggregate_trades
FILENAME_PATTERN = re.compile(
    r"^(?P<prefix>bn_aggtrades)_" # Matches the fixed prefix "bn_aggtrades_"
    r"(?P<symbol>[A-Z0-9]+)_"      # Matches the symbol (e.g., BTCUSDT)
    r"(?P<start_year>\d{4})-(?P<start_month>\d{2})-(?P<start_day>\d{2})_" # Matches YYYY-MM-DD
    r"to_"                         # Matches "_to_"
    r"(?P<end_year>\d{4})-(?P<end_month>\d{2})-(?P<end_day>\d{2})"     # Matches YYYY-MM-DD
    r"\.parquet$"                  # Matches ".parquet" extension
)


def parse_filename_for_metadata(filename):
    """
    Parses the filename (e.g., bn_aggtrades_BTCUSDT_2024-01-01_to_2024-01-03.parquet)
    to extract symbol, start time, and end time.
    Returns a dictionary with metadata or None if parsing fails.
    """
    match = FILENAME_PATTERN.match(os.path.basename(filename))
    if not match:
        return None
    
    data = match.groupdict()
    try:
        # For aggregate trades, utils.py uses YYYY-MM-DD dates for the filename.
        # The times are implicit (start of day for start_date, end of day for end_date)
        # when passing to client.get_aggregate_trades without explicit times in the filename specifier.
        # So we infer 00:00:00 for start, and 23:59:59.999 for end of day.
        # However, for the purpose of matching the *requested* interval, it's simpler
        # to just use the start of day for both if hours aren't specified.
        # If utils.py fetches exactly to '2024-01-03 00:00:00', the filename reflects '2024-01-03'.
        # We'll parse the date part and assume start-of-day for precision.
        start_time = datetime(
            int(data['start_year']), int(data['start_month']), int(data['start_day']),
            0, 0, 0, # Default to 00:00:00 for time if not in filename
            tzinfo=timezone.utc
        )
        end_time = datetime(
            int(data['end_year']), int(data['end_month']), int(data['end_day']),
            0, 0, 0, # Default to 00:00:00 for time if not in filename
            tzinfo=timezone.utc
        ) + timedelta(days=1) - timedelta(microseconds=1) # Set to end of day (23:59:59.999...)
        
        return {
            "symbol": data['symbol'],
            "start_time_utc": start_time,
            "end_time_utc": end_time,
            "prefix": data['prefix']
        }
    except ValueError:
        return None

def check_cache_file(filepath):
    """
    Performs various checks on a single Parquet cache file.
    Returns True if all checks pass, False otherwise.
    """
    print(f"\n--- Checking File: {filepath} ---")
    if not os.path.exists(filepath):
        print("  ERROR: File does not exist.")
        return False

    # 1. Check file size (basic check)
    file_size_bytes = os.path.getsize(filepath)
    if file_size_bytes == 0:
        print("  ERROR: File is empty (0 bytes).")
        return False
    print(f"  File Size: {file_size_bytes / 1024:.2f} KB")

    # 2. Try to parse filename for metadata
    metadata = parse_filename_for_metadata(filepath)
    if metadata:
        print(f"  Parsed Metadata: Symbol={metadata['symbol']}, Start={metadata['start_time_utc']}, End={metadata['end_time_utc']}")
    else:
        print("  WARNING: Could not parse filename for metadata. Timestamp range checks will be skipped.")
        print(f"  Filename: {os.path.basename(filepath)}") # Added for debug
        print(f"  Pattern: {FILENAME_PATTERN.pattern}") # Added for debug


    # 3. Readability and Basic Structure
    try:
        df = pd.read_parquet(filepath)
        print(f"  Successfully read Parquet file. Shape: {df.shape}")
        if df.empty:
            print("  WARNING: DataFrame is empty after reading. (May be valid for periods with no trades).")
    except Exception as e:
        print(f"  ERROR: Could not read Parquet file: {e}")
        traceback.print_exc()
        return False

    all_checks_ok = True

    # 4. Column Presence and Unexpected Columns
    expected_cols_set = set(EXPECTED_COLUMNS_INFO.keys())
    actual_cols_set = set(df.columns)

    missing_cols = expected_cols_set - actual_cols_set
    extra_cols = actual_cols_set - expected_cols_set

    if missing_cols:
        print(f"  ERROR: Missing expected columns: {missing_cols}")
        all_checks_ok = False
    if extra_cols:
        print(f"  WARNING: Found unexpected extra columns: {extra_cols}")

    # 5. Data Type, NaN, and Specific Value Checks
    for col_name, info in EXPECTED_COLUMNS_INFO.items():
        if col_name not in df.columns:
            continue

        # Check Dtype
        expected_dtype = pd.Series([], dtype=info['dtype']).dtype
        actual_dtype = df[col_name].dtype
        
        # More robust dtype comparison for datetime64[ns, UTC]
        if 'datetime64' in str(info['dtype']):
            if not pd.api.types.is_datetime64_any_dtype(actual_dtype):
                print(f"  ERROR: Column '{col_name}' dtype is '{actual_dtype}' but expected datetime type.")
                all_checks_ok = False
            elif 'UTC' in str(info['dtype']) and df[col_name].dt.tz is None:
                print(f"  ERROR: Column '{col_name}' dtype is '{actual_dtype}' (naive) but expected UTC-aware.")
                all_checks_ok = False
            elif 'UTC' in str(info['dtype']) and str(df[col_name].dt.tz) != 'UTC':
                 print(f"  ERROR: Column '{col_name}' dtype is '{actual_dtype}' with tz '{df[col_name].dt.tz}' but expected 'UTC'.")
                 all_checks_ok = False
        elif actual_dtype != expected_dtype:
            print(f"  ERROR: Column '{col_name}' dtype is '{actual_dtype}' but expected '{expected_dtype}'.")
            all_checks_ok = False

        # Check for NaNs if column is expected to be not_null
        if info.get('not_null', False):
            nan_count = df[col_name].isnull().sum()
            if nan_count > 0:
                print(f"  ERROR: Column '{col_name}' contains {nan_count} NaN/null values but is expected to be not-null.")
                all_checks_ok = False
        
        # Check for positive values if specified
        if info.get('check_positive', False) and pd.api.types.is_numeric_dtype(df[col_name]):
            if (df[col_name] <= 0).any():
                print(f"  WARNING: Column '{col_name}' contains zero or negative values but expected positive.")

        # Check for monotonic increasing if specified (typically for 'Timestamp')
        if info.get('monotonic_increasing', False):
            if not df[col_name].is_monotonic_increasing:
                print(f"  ERROR: Column '{col_name}' is not monotonically increasing.")
                all_checks_ok = False
    
    # 6. Timestamp Range Check (if metadata was parsed)
    if metadata and 'Timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Timestamp']):
        if not df.empty:
            min_time_in_df = df['Timestamp'].min()
            max_time_in_df = df['Timestamp'].max()
            
            # Ensure min_time_in_df is UTC if it's not already
            if min_time_in_df.tzinfo is None: min_time_in_df = min_time_in_df.replace(tzinfo=timezone.utc)
            if max_time_in_df.tzinfo is None: max_time_in_df = max_time_in_df.replace(tzinfo=timezone.utc)

            # Timestamps in data should be within the range [filename_start_time, filename_end_time]
            # Allow a small leeway (e.g., 1 second) if necessary due to millisecond precision or fetch boundaries
            leeway_seconds = 1 
            expected_start = metadata['start_time_utc']
            expected_end = metadata['end_time_utc']

            # Check if actual data range broadly covers the expected range
            # The logic here assumes utils.py fetches up to the end of the *day* for the end_date.
            # So, if a file is `2024-01-01_to_2024-01-03`, it should contain data for Jan 1, Jan 2, and Jan 3.
            # The start_time_utc in metadata is 00:00:00 on start date.
            # The end_time_utc in metadata is 23:59:59.999... on end date.

            if min_time_in_df < expected_start - timedelta(seconds=leeway_seconds):
                print(f"  WARNING: Minimum timestamp in data ({min_time_in_df}) is slightly before expected start from filename ({expected_start}). This might be acceptable due to data boundaries.")
            
            # The max time in data should be roughly up to the end of the specified end date.
            # If utils.py fetches data up to 2024-01-03 00:00:00, then the last trade would be at 2024-01-02 23:59:59.xxx
            # However, the filename is YYYY-MM-DD to YYYY-MM-DD. So the file should contain data *up to* the end of the end date.
            # The metadata's end_time_utc is now set to 23:59:59.999... of the end date.
            if max_time_in_df > expected_end + timedelta(seconds=leeway_seconds):
                print(f"  WARNING: Maximum timestamp in data ({max_time_in_df}) is slightly after expected end from filename ({expected_end}). This might be acceptable due to data boundaries.")
            
            # Additional check: Does the data span the *full* expected duration?
            # This is harder to check perfectly without knowing the exact granular fetching.
            # A simple check: min_time_in_df should be at or after expected_start.
            # max_time_in_df should be at or before expected_end.
            # If not, it's a concern.
            if min_time_in_df > expected_start + timedelta(minutes=1): # Allow a small margin
                print(f"  ERROR: First timestamp in data ({min_time_in_df}) is significantly after expected start from filename ({expected_start}). Data might be missing from start.")
                all_checks_ok = False
            
            if max_time_in_df < expected_end - timedelta(minutes=1): # Allow a small margin
                 print(f"  ERROR: Last timestamp in data ({max_time_in_df}) is significantly before expected end from filename ({expected_end}). Data might be missing from end.")
                 all_checks_ok = False

    if all_checks_ok:
        print("  All checks passed.")
    else:
        print("  One or more checks FAILED.")
    return all_checks_ok

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Check integrity of cached Parquet tick data files.")
    parser.add_argument(
        "filepath",
        nargs='?', # Makes the argument optional
        default=None,
        help="Path to a specific Parquet file to check. If not provided, all .parquet files in CACHE_DIR will be checked."
    )
    parser.add_argument(
        "--cache_dir",
        default=DEFAULT_CACHE_DIR,
        help=f"Directory containing cache files. Default: {DEFAULT_CACHE_DIR}"
    )
    args = parser.parse_args()

    cache_directory = args.cache_dir
    if not os.path.isdir(cache_directory):
        print(f"ERROR: Cache directory '{cache_directory}' does not exist.")
        exit(1)

    if args.filepath:
        if not os.path.isfile(args.filepath):
            print(f"ERROR: Specified file '{args.filepath}' does not exist.")
            exit(1)
        if not args.filepath.endswith(".parquet"):
            print(f"ERROR: Specified file '{args.filepath}' is not a .parquet file.")
            exit(1)
        check_cache_file(args.filepath)
    else:
        print(f"Checking all .parquet files in directory: {cache_directory}")
        found_files = [f for f in os.listdir(cache_directory) if f.endswith(".parquet")]
        if not found_files:
            print("No .parquet files found in the directory.")
        else:
            print(f"Found {len(found_files)} Parquet files.")
            overall_passed = 0
            overall_failed = 0
            for filename in found_files:
                full_path = os.path.join(cache_directory, filename)
                if check_cache_file(full_path):
                    overall_passed += 1
                else:
                    overall_failed += 1
            print(f"\n--- Overall Summary ---")
            print(f"Files Checked: {len(found_files)}")
            print(f"Passed: {overall_passed}")
            print(f"Failed: {overall_failed}")