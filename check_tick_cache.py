import os
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
import argparse
import re
import traceback
import time # Import time module for sleep

# --- Configuration ---
DEFAULT_CACHE_DIR = "./binance_data_cache/" # Adjusted default cache directory to match utils.py's default

# Expected columns and their properties (based on output of utils.py's fetch_continuous_aggregate_trades)
EXPECTED_COLUMNS_INFO = {
    'Price': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'Quantity': {'dtype': np.float64, 'not_null': True, 'check_positive': True},
    'IsBuyerMaker': {'dtype': np.bool_, 'not_null': False},
}

# Regex to parse filenames generated by the new daily fetching/caching in utils.py:
# bn_aggtrades_SYMBOL_YYYY-MM-DD_to_YYYY-MM-DD.parquet
FILENAME_PATTERN = re.compile(
    r"^(?P<prefix>bn_aggtrades)_" # Matches the fixed prefix "bn_aggtrades_"
    r"(?P<symbol>[A-Z0-9]+)_"      # Matches the symbol (e.g., BTCUSDT)
    r"(?P<start_year>\d{4})-(?P<start_month>\d{2})-(?P<start_day>\d{2})_" # Matches YYYY-MM-DD
    r"to_"                         # Matches "_to_"
    r"(?P<end_year>\d{4})-(?P<end_month>\d{2})-(?P<end_day>\d{2})"     # Matches YYYY-MM-DD
    r"\.parquet$"                  # Matches ".parquet" extension
)


def parse_filename_for_metadata(filename):
    """
    Parses the filename (e.g., bn_aggtrades_BTCUSDT_2024-01-01_to_2024-01-01.parquet)
    to extract symbol, start date, and end date.
    Returns a dictionary with metadata or None if parsing fails.
    """
    match = FILENAME_PATTERN.match(os.path.basename(filename))
    if not match:
        return None
    
    data = match.groupdict()
    try:
        start_time = datetime(
            int(data['start_year']), int(data['start_month']), int(data['start_day']),
            0, 0, 0, # 00:00:00
            tzinfo=timezone.utc
        )
        end_time = datetime(
            int(data['end_year']), int(data['end_month']), int(data['end_day']),
            23, 59, 59, 999999, # 23:59:59.999999
            tzinfo=timezone.utc
        )
        
        return {
            "symbol": data['symbol'],
            "start_time_utc": start_time,
            "end_time_utc": end_time,
            "prefix": data['prefix']
        }
    except ValueError:
        return None

def validate_daily_data(filepath: str) -> tuple[bool, str]:
    """
    Performs various checks on a single daily Parquet cache file for aggregate trades.
    Includes retry logic for reading the file in case of file system delays.
    Returns True and a success message if all checks pass, False and an error message otherwise.
    """
    print(f"\n--- Checking File: {filepath} ---")
    
    df = pd.DataFrame() # Initialize df
    max_read_retries = 10
    read_retry_delay_sec = 0.2 # 200 milliseconds

    # 1. Readability and Basic Structure (with retry logic)
    for i in range(max_read_retries):
        try:
            df = pd.read_parquet(filepath)
            print(f"  Successfully read Parquet file after {i+1} attempt(s). Shape: {df.shape}")
            if df.empty:
                return True, "DataFrame is empty after reading. (May be valid for periods with no trades)."
            break # Exit retry loop if successful
        except FileNotFoundError:
            print(f"  WARNING: File not found during read attempt {i+1}/{max_read_retries}. Retrying in {read_retry_delay_sec}s...")
            time.sleep(read_retry_delay_sec)
        except Exception as e:
            traceback.print_exc()
            return False, f"Could not read Parquet file (error type: {type(e).__name__}): {e}"
    else: # This else block executes if the loop completes without a 'break'
        return False, f"File did not become readable after {max_read_retries} attempts. File not found or persistently inaccessible."

    # 2. Check file size (basic check, now done after successful read)
    file_size_bytes = os.path.getsize(filepath)
    if file_size_bytes == 0 and not df.empty: # if df is not empty, but size is 0 bytes, this is an inconsistency
        return False, "File is 0 bytes but DataFrame was not empty after reading (inconsistent)."
    print(f"  File Size: {file_size_bytes / 1024:.2f} KB")


    # 3. Try to parse filename for metadata
    metadata = parse_filename_for_metadata(filepath)
    if metadata:
        print(f"  Parsed Metadata: Symbol={metadata['symbol']}, Start={metadata['start_time_utc']}, End={metadata['end_time_utc']}")
    else:
        return False, f"Could not parse filename '{os.path.basename(filepath)}' for metadata. It does not match the expected pattern."

    all_checks_ok = True
    messages = []

    # Check the DataFrame index (expected to be 'Timestamp')
    if not isinstance(df.index, pd.DatetimeIndex):
        messages.append(f"Index is not a DatetimeIndex. Actual type: {type(df.index)}")
        all_checks_ok = False
    elif df.index.tz is None or str(df.index.tz) != 'UTC':
        messages.append(f"Index is not UTC-aware or not UTC. Actual tz: {df.index.tz}")
        all_checks_ok = False
    elif not df.index.is_monotonic_increasing:
        messages.append("Index (Timestamp) is not monotonically increasing.")
        all_checks_ok = False
    
    # 4. Column Presence and Unexpected Columns (excluding the 'Timestamp' index)
    expected_cols_set = set(EXPECTED_COLUMNS_INFO.keys())
    actual_cols_set = set(df.columns)

    missing_cols = expected_cols_set - actual_cols_set
    extra_cols = actual_cols_set - expected_cols_set

    if missing_cols:
        messages.append(f"Missing expected columns: {missing_cols}")
        all_checks_ok = False
    if extra_cols:
        messages.append(f"WARNING: Found unexpected extra columns: {extra_cols}")

    # 5. Data Type, NaN, and Specific Value Checks for actual columns
    for col_name, info in EXPECTED_COLUMNS_INFO.items():
        if col_name not in df.columns:
            continue # Already reported if missing

        # Check Dtype
        expected_dtype = pd.Series([], dtype=info['dtype']).dtype
        actual_dtype = df[col_name].dtype
        
        if actual_dtype != expected_dtype:
            messages.append(f"Column '{col_name}' dtype is '{actual_dtype}' but expected '{expected_dtype}'.")
            all_checks_ok = False

        # Check for NaNs if column is expected to be not_null
        if info.get('not_null', False):
            nan_count = df[col_name].isnull().sum()
            if nan_count > 0:
                messages.append(f"Column '{col_name}' contains {nan_count} NaN/null values but is expected to be not-null.")
                all_checks_ok = False
        
        # Check for positive values if specified
        if info.get('check_positive', False) and pd.api.types.is_numeric_dtype(df[col_name]):
            if (df[col_name] <= 0).any():
                messages.append(f"WARNING: Column '{col_name}' contains zero or negative values but expected positive.")

    # 6. Timestamp Range Check (using the DataFrame's index and parsed metadata)
    if not df.empty:
        min_time_in_df = df.index.min()
        max_time_in_df = df.index.max()
        
        # Ensure times are UTC for comparison
        if min_time_in_df.tzinfo is None: min_time_in_df = min_time_in_df.tz_localize('UTC')
        if max_time_in_df.tzinfo is None: max_time_in_df = max_time_in_df.tz_localize('UTC')

        expected_start = metadata['start_time_utc']
        expected_end = metadata['end_time_utc']
        
        # Allow a small leeway (e.g., 1 second) for boundaries due to API/timestamp precision
        leeway = timedelta(seconds=1) 

        if min_time_in_df < expected_start - leeway:
            messages.append(f"WARNING: First timestamp in data ({min_time_in_df}) is slightly before expected start from filename ({expected_start}).")
        elif min_time_in_df > expected_start + timedelta(minutes=5): # More strict check for start
            messages.append(f"ERROR: First timestamp in data ({min_time_in_df}) is significantly after expected start from filename ({expected_start}). Data might be missing from start.")
            all_checks_ok = False
        
        if max_time_in_df > expected_end + leeway:
            messages.append(f"WARNING: Maximum timestamp in data ({max_time_in_df}) is slightly after expected end from filename ({expected_end}).")
        elif max_time_in_df < expected_end - timedelta(minutes=5): # More strict check for end
            messages.append(f"ERROR: Last timestamp in data ({max_time_in_df}) is significantly before expected end from filename ({expected_end}). Data might be missing from end.")
            all_checks_ok = False
        
        if df.shape[0] > 1:
            time_diffs = df.index.to_series().diff().dropna()
            max_gap = time_diffs.max()
            if max_gap > timedelta(minutes=5): # Example threshold for large gap
                messages.append(f"WARNING: Large time gap detected in data: {max_gap}. Data might be incomplete for the day.")


    if all_checks_ok:
        return True, "All checks passed."
    else:
        return False, "One or more checks FAILED:\n" + "\n".join([f"  - {msg}" for msg in messages])

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Check integrity of cached Parquet tick data files.")
    parser.add_argument(
        "filepath",
        nargs='?', # Makes the argument optional
        default=None,
        help="Path to a specific Parquet file to check. If not provided, all .parquet files in CACHE_DIR will be checked."
    )
    parser.add_argument(
        "--cache_dir",
        default=DEFAULT_CACHE_DIR,
        help=f"Directory containing cache files. Default: {DEFAULT_CACHE_DIR}"
    )
    args = parser.parse_args()

    cache_directory = args.cache_dir
    if not os.path.isdir(cache_directory):
        print(f"ERROR: Cache directory '{cache_directory}' does not exist.")
        exit(1)

    if args.filepath:
        if not os.path.isfile(args.filepath):
            print(f"ERROR: Specified file '{args.filepath}' does not exist.")
            exit(1)
        if not args.filepath.endswith(".parquet"):
            print(f"ERROR: Specified file '{args.filepath}' is not a .parquet file.")
            exit(1)
        
        # Call the new validation function
        is_valid, msg = validate_daily_data(args.filepath)
        print(msg)
        if not is_valid:
            exit(1) # Exit with error code if validation fails
    else:
        print(f"Checking all .parquet files in directory: {cache_directory}")
        found_files = []
        # Walk through subdirectories (e.g., symbol directories) to find parquet files
        for root, _, files in os.walk(cache_directory):
            for f in files:
                if f.endswith(".parquet"):
                    found_files.append(os.path.join(root, f))

        if not found_files:
            print("No .parquet files found in the directory or its subdirectories.")
        else:
            print(f"Found {len(found_files)} Parquet files.")
            overall_passed = 0
            overall_failed = 0
            for filepath in found_files:
                is_valid, msg = validate_daily_data(filepath)
                print(msg) # Print validation message for each file
                if is_valid:
                    overall_passed += 1
                else:
                    overall_failed += 1
            print(f"\n--- Overall Summary ---")
            print(f"Files Checked: {len(found_files)}")
            print(f"Passed: {overall_passed}")
            print(f"Failed: {overall_failed}")
            if overall_failed > 0:
                exit(1) # Indicate failure if any file failed validation