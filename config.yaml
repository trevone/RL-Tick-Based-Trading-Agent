# config.yaml

agent_type: "PPO"

binance_settings: 
  testnet: false 

run_settings:
  log_level: "normal"
  model_name: "simple_sequence_env"
  # Set to false if you want to start a completely new model training session
  continue_from_existing_model: true
  default_symbol: "BTCUSDT"
  historical_interval: "1h"
  start_date_train: "2024-11-06 00:00:00"
  end_date_train: "2025-04-21 23:59:59"
  start_date_eval: "2025-04-22 00:00:00"
  end_date_eval: "2025-06-17 23:59:59"
  eval_freq_episodes: 1
  stop_training_patience_evals: 5

environment:
  env_type: "simple_sequence_env"
  # --- Parameters for the HybridRewardEnv ---
  reward_at_entry: 1.0  # The maximum reward for holding at the entry price
  decay_rate: 0.1       # Controls how quickly the hold reward decreases



  # --- General Environment Parameters ---
  kline_window_size: 50
  kline_price_features:
    Open: {}
    High: {}
    Low: {}
    Close: {}
    Volume: {}
    RSI_14:
      function: "RSI"
      params:
        timeperiod: 14
    BBANDS_Upper:
      function: "BBANDS"
      params:
        timeperiod: 20
        nbdevup: 2
        nbdevdn: 2
      output_field: 0
    BBANDS_Lower:
      function: "BBANDS"
      params:
        timeperiod: 20
        nbdevup: 2
        nbdevdn: 2
      output_field: 2
    # Example for a more complex indicator:
    # SMA_20: {"period": 20}

  tick_feature_window_size: 15
  live_ta_recalc_every_n_steps: 1
 
  
  tick_resample_interval_ms: 60000

ppo_params:
  total_timesteps: 10000000
  learning_rate: 0.0003
  n_steps: 4096
  batch_size: 128
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  # --- NEW: INCREASED EXPLORATION ---
  # Forces the agent to try new things and break out of the cowardly loop.
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.02
  policy_kwargs:
    net_arch: [128, 64]
