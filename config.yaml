# config.yaml

agent_type: "PPO"

binance_settings: 
  testnet: false 

run_settings:
  log_level: "normal"
  model_name: "profit_driven_env"
  # Set to false if you want to start a completely new model training session
  continue_from_existing_model: true
  default_symbol: "BTCUSDT"
  historical_interval: "1h"
  start_date_train: "2024-11-06 00:00:00"
  end_date_train: "2025-04-21 23:59:59"
  start_date_eval: "2025-04-22 00:00:00"
  end_date_eval: "2025-06-17 23:59:59"
  eval_freq_episodes: 1
  stop_training_patience_evals: 5

environment:
  env_type: "profit_driven_env"
  # --- Parameters for the HybridRewardEnv ---
  reward_at_entry: 1.0  # The maximum reward for holding at the entry price
  decay_rate: 0.1       # Controls how quickly the hold reward decreases



  # --- General Environment Parameters ---
  kline_window_size: 70
  kline_price_features:
    Open: {}
    High: {}
    Low: {}
    Close: {}
    Volume: {}
    STOCH_SlowK:
      function: "STOCH"
      params:
        fastk_period: 5
        slowk_period: 3
        slowk_matype: 0
        slowd_period: 3
        slowd_matype: 0
      output_field: 0

    STOCH_SlowD:
      function: "STOCH"
      params:
        fastk_period: 5
        slowk_period: 3
        slowk_matype: 0
        slowd_period: 3
        slowd_matype: 0
      output_field: 1
    MACD_Line:
      function: "MACD"
      params:
        fastperiod: 12
        slowperiod: 26
        signalperiod: 9
      output_field: 0

    MACD_Signal:
      function: "MACD"
      params:
        fastperiod: 12
        slowperiod: 26
        signalperiod: 9
      output_field: 1

    MACD_Hist:
      function: "MACD"
      params:
        fastperiod: 12
        slowperiod: 26
        signalperiod: 9
      output_field: 2

  tick_feature_window_size: 50
  live_ta_recalc_every_n_steps: 15
 
  
  tick_resample_interval_ms: 60000

ppo_params:
  total_timesteps: 1000000
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  # --- NEW: INCREASED EXPLORATION ---
  # Forces the agent to try new things and break out of the cowardly loop.
  ent_coef: 0.03
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.02
  policy_kwargs:
    net_arch: [128, 64]
