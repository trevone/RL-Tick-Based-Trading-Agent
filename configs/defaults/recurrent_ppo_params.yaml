# configs/defaults/recurrent_ppo_params.yaml
# RecurrentPPO (PPO with LSTM Policy) specific hyperparameters from sb3_contrib
recurrent_ppo_params:
  total_timesteps: 10000000           # Total number of samples to train on
  learning_rate: 0.0003               # Learning rate for the Adam optimizer
  n_steps: 2048                       # The number of steps to run for each environment per update
  batch_size: 64                      # Minibatch size
  n_epochs: 10                        # Number of epoch when optimizing the surrogate loss
  gamma: 0.99                         # Discount factor
  gae_lambda: 0.95                    # Factor for trade-off of bias vs variance for GAE
  clip_range: 0.2                     # Clipping parameter for PPO
  ent_coef: 0.01                      # Entropy coefficient for the loss calculation
  vf_coef: 0.5                        # Value function coefficient for the loss calculation
  max_grad_norm: 0.5                  # The maximum value for the gradient clipping
  use_sde: False                      # RecurrentPPO typically does not use SDE
  policy_kwargs: "{'n_lstm_layers': 2, 'lstm_hidden_size': 128, 'enable_critic_lstm': True, 'shared_extractors': True, 'net_arch': dict(pi=[], vf=[])}" # LSTM specific architecture (as string)
                                      # 'net_arch': dict(pi=[], vf=[]) means the MlpLstmPolicy will define its own feedforward parts.