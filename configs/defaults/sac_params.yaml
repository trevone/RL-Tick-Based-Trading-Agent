# configs/defaults/sac_params.yaml
# SAC (Soft Actor-Critic) specific hyperparameters
sac_params:
  total_timesteps: 1000000            # Total number of samples to train on
  learning_rate: 0.0003               # Learning rate for actor and critic (and alpha)
  buffer_size: 1000000                # Size of the replay buffer (max number of transitions)
  learning_starts: 10000              # Number of steps before learning starts
  batch_size: 256                     # Minibatch size for SAC updates
  tau: 0.005                          # Soft update coefficient (for the target network)
  gamma: 0.99                         # Discount factor
  train_freq: "[1, 'episode']"        # Update the agent every `train_freq` steps. (int or tuple as string)
  gradient_steps: 1                   # How many gradient steps to do after each rollout.
  ent_coef: "auto"                    # Entropy regularization coefficient. "auto" or float.
  use_sde: True                       # Whether to use generalized State Dependent Exploration (SDE)
  sde_sample_freq: -1                 # Sample a new SDE noise every n steps (and for the initial exploration)
  policy_kwargs: "{'net_arch': [256, 256]}" # Network architecture for the policy and Q-functions (as string)